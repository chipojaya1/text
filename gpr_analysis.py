# -*- coding: utf-8 -*-
"""GPR_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14OdgJHbbEjqyKuxYictC8alAulQdRdaZ
"""

import os
import re
import glob
import pandas as pd
import xml.etree.ElementTree as ET
from datetime import datetime
from typing import Dict, Set, Tuple, List, Optional
from bs4 import BeautifulSoup
from tqdm import tqdm

# ---------------------------------------------------
# CONFIGURATION
# ---------------------------------------------------

XML_DIR = "/content/2016_data/2016"          # folder with XML transcript files

# Dictionary files
GPR_DICT_FILE = "/content/GPR dictionary.csv"          # columns: term, category
COUNTRY_DICT_FILE = "/content/country_variants.csv"    # columns: country, variant

# Output directory
OUTPUT_DIR = "/content/output"

# Column names in dictionary files
GPR_TERM_COL = "term"
GPR_CAT_COL = "category"            # if absent, categories will be blank
COUNTRY_CODE_COL = "country"
COUNTRY_VARIANT_COL = "variant"

# GPR discussion block window (±K)
GPR_WINDOW_K = 3

# Year range for filtered analysis
YEAR_FILTER_START = 2016
YEAR_FILTER_END = 2017

# ---------------------------------------------------
# TEXT UTILITIES
# ---------------------------------------------------

def clean_text(text: str) -> str:
    """Remove extra whitespace and normalize text."""
    if text is None:
        return ""
    text = text.replace("\r", " ").replace("\n", " ")
    text = re.sub(r"\s+", " ", text)
    return text.strip()


def normalize_lower(text: str) -> str:
    """Clean and lowercase text."""
    return clean_text(text).lower()


def split_into_sentences(text: str) -> List[str]:
    """
    Split text into sentences using a simple regex.
    (Consistent + deterministic for RA work.)
    """
    text = text.strip()
    if not text:
        return []
    sentence_end_re = re.compile(r"(?<=[.?!])\s+")
    sents = sentence_end_re.split(text)
    return [s.strip() for s in sents if s.strip()]


def tokenize_words(sentence: str) -> List[str]:
    """Tokenize sentence into words."""
    cleaned = re.sub(r"[^a-zA-Z0-9']+", " ", sentence)
    return [t for t in cleaned.split() if t]


def count_words(sentence: str) -> int:
    return len(tokenize_words(sentence))


def compile_term_regex(terms: Set[str]) -> re.Pattern:
    """
    Compiled regex matching any term in 'terms', whole-word.
    Multi-word phrases are also supported.
    """
    if not terms:
        return re.compile(r"(?!x)x")  # matches nothing

    # longest-first improves multi-word/phrase matches
    escaped = sorted((re.escape(t) for t in terms), key=len, reverse=True)
    pattern = r"\b(?:" + "|".join(escaped) + r")\b"
    return re.compile(pattern, re.IGNORECASE)


# ---------------------------------------------------
# LOAD DICTIONARIES (FIXED)
# ---------------------------------------------------

# --------------------------------------------------
# Load GPR dictionary (UPDATED FORMAT)
# --------------------------------------------------

import pandas as pd

gpr_raw = pd.read_csv("/content/GPR dictionary.csv", encoding="cp1252", header=None)

# Column 2 contains comma-separated keyword phrases
# Rows 2 onward contain actual content
gpr_terms = (
    gpr_raw.iloc[2:, 2]
    .dropna()
    .astype(str)
    .str.lower()
    .str.split(",")
    .explode()
    .str.strip()
    .unique()
    .tolist()
)

print(f"Loaded {len(gpr_terms)} GPR terms")
print("Sample GPR terms:", gpr_terms[:20])


def load_country_lexicon(path: str) -> Tuple[Dict[str, str], Set[str]]:
    """
    Load country variant dictionary.
    """
    df = None
    if path.lower().endswith((".xlsx", ".xls")):
        df = pd.read_excel(path)
    else:
        for enc in ['utf-8', 'cp1252', 'latin1']:
            try:
                df = pd.read_csv(path, sep=None, engine="python", encoding=enc)
                break
            except UnicodeDecodeError:
                continue

    if df is None:
        raise ValueError(f"Could not read {path} - encoding error.")

    # validate columns, fallback if needed
    if COUNTRY_CODE_COL not in df.columns or COUNTRY_VARIANT_COL not in df.columns:
        # simple fallback if header missing but 2 columns exist
        if len(df.columns) >= 2:
             df.columns = [COUNTRY_CODE_COL, COUNTRY_VARIANT_COL] + list(df.columns[2:])
        else:
             raise ValueError(f"Country file invalid. Found columns: {list(df.columns)}")

    df = df[[COUNTRY_CODE_COL, COUNTRY_VARIANT_COL]].dropna().copy()
    df[COUNTRY_CODE_COL] = df[COUNTRY_CODE_COL].astype(str).str.strip().str.lower()
    df[COUNTRY_VARIANT_COL] = df[COUNTRY_VARIANT_COL].astype(str).str.strip().str.lower()

    variant_to_country: Dict[str, str] = {}
    for _, r in df.iterrows():
        variant_to_country[r[COUNTRY_VARIANT_COL]] = r[COUNTRY_CODE_COL]

    all_variants = set(variant_to_country.keys())
    print(f"Loaded {len(all_variants)} country variants")
    return variant_to_country, all_variants


# ---------------------------------------------------
# XML PARSING
# ---------------------------------------------------

def extract_raw_body_from_xml(xml_path: str) -> str:
    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()
        body_elem = root.find(".//Body")
        if body_elem is not None and body_elem.text:
            return body_elem.text
    except ET.ParseError as e:
        print(f"Warning: XML parse error in {xml_path}: {e}")
    return ""


def extract_management_text_from_xml(xml_path: str) -> str:
    """
    Extract management presentation section from XML transcript.
    Uses text between 'Presentation' and 'Questions and Answers' markers.
    """
    raw_body = extract_raw_body_from_xml(xml_path)
    if not raw_body:
        return ""

    body_lower = raw_body.lower()
    idx_pres = body_lower.find("presentation")
    idx_qna = body_lower.find("questions and answers")

    start_idx = idx_pres if idx_pres != -1 else 0
    end_idx = idx_qna if idx_qna != -1 else len(raw_body)

    mgmt_raw = raw_body[start_idx:end_idx]
    return normalize_lower(mgmt_raw)


def extract_call_date_from_xml(xml_path: str) -> Optional[str]:
    """
    Extract call date from <startDate>.
    Returns YYYY-MM-DD if parseable else None.
    """
    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()
        start_elem = root.find(".//startDate")
        if start_elem is not None and start_elem.text:
            raw = start_elem.text.strip()
            raw = re.sub(r"\s*(GMT|EST|PST|UTC|ET|PT).*$", "", raw, flags=re.IGNORECASE)

            for fmt in ["%d-%b-%y %I:%M%p", "%d-%b-%Y %I:%M%p", "%Y-%m-%d", "%d-%b-%y"]:
                try:
                    dt = datetime.strptime(raw.strip(), fmt)
                    return dt.strftime("%Y-%m-%d")
                except ValueError:
                    continue
    except Exception:
        pass
    return None


def extract_firm_id_from_xml(xml_path: str) -> str:
    """
    Extract firm identifier from <companyTicker> else filename stub.
    """
    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()
        ticker = root.find(".//companyTicker")
        if ticker is not None and ticker.text:
            return ticker.text.strip()
    except Exception:
        pass
    return os.path.basename(xml_path).replace("_T.xml", "").replace(".xml", "")


def extract_year_from_date(date_str: Optional[str]) -> Optional[int]:
    if date_str and len(date_str) >= 4:
        try:
            return int(date_str[:4])
        except ValueError:
            return None
    return None


# ---------------------------------------------------
# MATCHING HELPERS
# ---------------------------------------------------

def find_terms(sentence: str, term_regex: re.Pattern) -> List[str]:
    if not sentence:
        return []
    matches = term_regex.findall(sentence)
    return sorted(set(m.lower() for m in matches))


def find_country_mentions(sentence: str,
                          variant_to_country: Dict[str, str],
                          country_regex: re.Pattern) -> Set[str]:
    """
    Finds country mentions in a sentence using variant regex.
    Returns standardized country codes/names (lower).
    """
    found = set()
    if not sentence:
        return found

    matches = country_regex.findall(sentence.lower())
    for m in matches:
        c = variant_to_country.get(m.lower())
        if c:
            found.add(c)
    return found


# ---------------------------------------------------
# CORE: BUILD SENTENCE-LEVEL + APPLY ±K DISCUSSION BLOCKS
# ---------------------------------------------------

def build_sentence_level_dataset(xml_dir: str,
                                 gpr_regex: re.Pattern,
                                 country_regex: re.Pattern,
                                 variant_to_country: Dict[str, str],
                                 term_to_category: Dict[str, str],
                                 k_window: int = 3) -> pd.DataFrame:
    """
    Build sentence-level dataset for ALL management presentation sentences.

    Steps per transcript:
      - split management presentation into sentences
      - anchor sentence if contains GPR keyword
      - mark is_gpr_sentence = 1 for any sentence within ±K of any anchor
      - detect countries (reported for every sentence; used later for country pairs only within GPR sentences)
    """
    rows = []

    xml_files = sorted(glob.glob(os.path.join(xml_dir, "*_T.xml")))
    if not xml_files:
        # fallback
        xml_files = sorted(glob.glob(os.path.join(xml_dir, "*.xml")))

    print(f"Processing {len(xml_files)} XML files from {xml_dir}...")

    for idx_file, xml_path in enumerate(xml_files, start=1):
        transcript_id = os.path.basename(xml_path).replace(".xml", "")
        firm_id = extract_firm_id_from_xml(xml_path)
        call_date = extract_call_date_from_xml(xml_path)
        year = extract_year_from_date(call_date)

        mgmt_text = extract_management_text_from_xml(xml_path)
        sentences = split_into_sentences(mgmt_text)

        # anchor positions
        anchor_positions = []
        sent_terms: List[List[str]] = []
        sent_cats: List[List[str]] = []
        sent_countries: List[Set[str]] = []
        sent_wordcounts: List[int] = []

        for s_i, sent in enumerate(sentences):
            wc = count_words(sent)
            sent_wordcounts.append(wc)

            terms_found = find_terms(sent, gpr_regex)
            sent_terms.append(terms_found)

            if terms_found:
                anchor_positions.append(s_i)

            cats_found = sorted(set(term_to_category.get(t, "") for t in terms_found if term_to_category.get(t, "")))
            sent_cats.append(cats_found)

            countries_found = find_country_mentions(sent, variant_to_country, country_regex)
            sent_countries.append(countries_found)

        # compute discussion block marks
        gpr_block_positions = set()
        if anchor_positions:
            max_idx = len(sentences) - 1
            for a in anchor_positions:
                lo = max(0, a - k_window)
                hi = min(max_idx, a + k_window)
                for j in range(lo, hi + 1):
                    gpr_block_positions.add(j)

        # emit rows
        for s_i, sent in enumerate(sentences):
            anchor = 1 if s_i in anchor_positions else 0
            is_gpr_sentence = 1 if s_i in gpr_block_positions else 0

            rows.append({
                "firm_id": firm_id,
                "transcript_id": transcript_id,
                "call_date": call_date,
                "year": year,

                "sentence_index": s_i,      # 0-based
                "sentence_id": s_i + 1,     # 1-based
                "sentence_text": sent,
                "word_count": sent_wordcounts[s_i],

                # Anchor: direct keyword hit
                "contains_gpr_keyword": anchor,
                "gpr_terms": "|".join(sent_terms[s_i]) if sent_terms[s_i] else "",
                "gpr_categories": "|".join(sent_cats[s_i]) if sent_cats[s_i] else "",

                # Block sentence: ±K around anchors
                "is_gpr_sentence": is_gpr_sentence,

                # Countries (full sentence-level reporting)
                "countries_mentioned": "|".join(sorted(sent_countries[s_i])) if sent_countries[s_i] else "",
                "country_count": len(sent_countries[s_i]),
            })

    return pd.DataFrame(rows)


# ---------------------------------------------------
# OUTPUT 2: GPR sentences × country pairs
# ---------------------------------------------------

def build_gpr_sentences_country(df_sent: pd.DataFrame) -> pd.DataFrame:
    """
    One row per (GPR sentence, country) pair.
    Uses is_gpr_sentence=1 (discussion blocks), and requires country_count>0.
    """
    rows = []
    df = df_sent[(df_sent["is_gpr_sentence"] == 1) & (df_sent["country_count"] > 0)].copy()

    for _, r in df.iterrows():
        countries = (r["countries_mentioned"] or "").split("|")
        for c in countries:
            c = c.strip()
            if not c:
                continue
            rows.append({
                "firm_id": r["firm_id"],
                "transcript_id": r["transcript_id"],
                "call_date": r["call_date"],
                "year": r["year"],
                "sentence_id": r["sentence_id"],
                "sentence_text": r["sentence_text"],
                "word_count": r["word_count"],
                "contains_gpr_keyword": r["contains_gpr_keyword"],
                "gpr_terms": r["gpr_terms"],
                "gpr_categories": r["gpr_categories"],
                "country": c
            })

    return pd.DataFrame(rows)


# ---------------------------------------------------
# OUTPUT 3: firm-country-year indices
# ---------------------------------------------------

def build_firm_country_year(df_sent: pd.DataFrame, df_sc: pd.DataFrame) -> pd.DataFrame:
    """
    I_GPR_ict: 1 if any GPR sentence references country c
    Frac_GPR_ict: (# GPR sentences referencing country c) / (total sentences in presentation)
    Word_GPR_ict: (# words in GPR sentences referencing country c) / (total words in presentation)
    """
    totals = df_sent.groupby(["firm_id", "year"], dropna=False).agg(
        total_sentences=("sentence_id", "count"),
        total_words=("word_count", "sum")
    ).reset_index()

    if df_sc.empty:
        return pd.DataFrame(columns=[
            "firm_id", "year", "country",
            "gpr_sentence_count", "gpr_word_count",
            "total_sentences", "total_words",
            "I_GPR_ict", "Frac_GPR_ict", "Word_GPR_ict"
        ])

    # count unique sentence occurrences per firm-year-country
    country_agg = df_sc.groupby(["firm_id", "year", "country"], dropna=False).agg(
        gpr_sentence_count=("sentence_id", "nunique"),
        gpr_word_count=("word_count", "sum")
    ).reset_index()

    df_fcy = country_agg.merge(totals, on=["firm_id", "year"], how="left")
    df_fcy["I_GPR_ict"] = (df_fcy["gpr_sentence_count"] > 0).astype(int)
    df_fcy["Frac_GPR_ict"] = df_fcy["gpr_sentence_count"] / df_fcy["total_sentences"]
    df_fcy["Word_GPR_ict"] = df_fcy["gpr_word_count"] / df_fcy["total_words"]
    return df_fcy


# ---------------------------------------------------
# OUTPUT 4: firm-year indices
# ---------------------------------------------------

def build_firm_year(df_sent: pd.DataFrame) -> pd.DataFrame:
    """
    I_GPR_it: 1 if any GPR sentence exists in firm-year
    Frac_GPR_it: (# GPR sentences) / (total sentences in presentation)
    Word_GPR_it: (# words in GPR sentences) / (total words in presentation)
    """
    totals = df_sent.groupby(["firm_id", "year"], dropna=False).agg(
        total_sentences=("sentence_id", "count"),
        total_words=("word_count", "sum")
    ).reset_index()

    gpr_df = df_sent[df_sent["is_gpr_sentence"] == 1].copy()
    gpr_agg = gpr_df.groupby(["firm_id", "year"], dropna=False).agg(
        gpr_sentence_count=("sentence_id", "count"),
        gpr_word_count=("word_count", "sum")
    ).reset_index()

    df_fy = totals.merge(gpr_agg, on=["firm_id", "year"], how="left")
    df_fy["gpr_sentence_count"] = df_fy["gpr_sentence_count"].fillna(0).astype(int)
    df_fy["gpr_word_count"] = df_fy["gpr_word_count"].fillna(0).astype(int)

    df_fy["I_GPR_it"] = (df_fy["gpr_sentence_count"] > 0).astype(int)
    df_fy["Frac_GPR_it"] = df_fy["gpr_sentence_count"] / df_fy["total_sentences"]
    df_fy["Word_GPR_it"] = df_fy["gpr_word_count"] / df_fy["total_words"]
    return df_fy


# ---------------------------------------------------
# MAIN
# ---------------------------------------------------

def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    print("=" * 70)
    print("Running GPR pipeline (±K discussion blocks) with country variants")
    print("=" * 70)

    print("\n[1/4] Loading dictionaries...")
    gpr_terms, term_to_category = load_gpr_dictionary(GPR_DICT_FILE)
    variant_to_country, all_variants = load_country_lexicon(COUNTRY_DICT_FILE)

    print("\n[2/4] Compiling regex patterns...")
    gpr_regex = compile_term_regex(gpr_terms)
    country_regex = compile_term_regex(all_variants)

    print("\n[3/4] Building sentence-level dataset...")
    df_sent = build_sentence_level_dataset(
        xml_dir=XML_DIR,
        gpr_regex=gpr_regex,
        country_regex=country_regex,
        variant_to_country=variant_to_country,
        term_to_category=term_to_category,
        k_window=GPR_WINDOW_K
    )

    print(f"  Total sentences: {len(df_sent)}")
    print(f"  Anchor (keyword-hit) sentences: {int(df_sent['contains_gpr_keyword'].sum())}")
    print(f"  GPR discussion-block sentences (±{GPR_WINDOW_K}): {int(df_sent['is_gpr_sentence'].sum())}")

    print("\n[4/4] Building remaining outputs...")
    df_sc = build_gpr_sentences_country(df_sent)
    df_fcy = build_firm_country_year(df_sent, df_sc)
    df_fy = build_firm_year(df_sent)

    # Save required files
    out_sentence = os.path.join(OUTPUT_DIR, "sentence_level.csv")
    out_sc = os.path.join(OUTPUT_DIR, "gpr_sentences_country.csv")
    out_fcy = os.path.join(OUTPUT_DIR, "firm_country_year.csv")
    out_fy = os.path.join(OUTPUT_DIR, "firm_year.csv")

    df_sent.to_csv(out_sentence, index=False)
    df_sc.to_csv(out_sc, index=False)
    df_fcy.to_csv(out_fcy, index=False)
    df_fy.to_csv(out_fy, index=False)

    print("\nSaved outputs:")
    print(" -", out_sentence)
    print(" -", out_sc)
    print(" -", out_fcy)
    print(" -", out_fy)

    print("\nQuick checks:")
    print("  sentence_level rows:", df_sent.shape)
    print("  gpr_sentences_country rows:", df_sc.shape)
    print("  firm_country_year rows:", df_fcy.shape)
    print("  firm_year rows:", df_fy.shape)

    print("\nDone.")


if __name__ == "__main__":
    main()

